{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_drama_writer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W-xnepXnUsd",
        "colab_type": "text"
      },
      "source": [
        "# RNN Drama Writer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnI443RlUJKx",
        "colab_type": "code",
        "outputId": "d4a63447-2df9-435f-fbce-3f19d6801795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoeRvnUbUaMM",
        "colab_type": "code",
        "outputId": "03f1f0ee-50e0-4119-d1cb-3f55d790e6e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY5xW2MmU_YM",
        "colab_type": "code",
        "outputId": "ee4e0369-8fc4-4cc0-d0cb-7af46559be2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l8vLlSlVBLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssw-COHAXFia",
        "colab_type": "code",
        "outputId": "d305942f-3cae-4e66-d81b-5a54a18fe19a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqgzyL70XI_Z",
        "colab_type": "code",
        "outputId": "542ee602-e3ed-4cf9-a9ff-9d57a184f832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaT3oTRlXZcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_9tJsp1aciY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c0sFkZOadaW",
        "colab_type": "code",
        "outputId": "b8025370-2014-42a9-94d1-3b1a9f4c1050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2u-WaC6ahfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o1yqh44j0kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av1q1DfpkPxA",
        "colab_type": "code",
        "outputId": "39415e0d-df69-4247-e771-0b59b3e94142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 172 steps\n",
            "Epoch 1/50\n",
            "172/172 [==============================] - 20s 117ms/step - loss: 2.5523\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 14s 81ms/step - loss: 1.8818\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 14s 82ms/step - loss: 1.6407\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 14s 83ms/step - loss: 1.5075\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 14s 83ms/step - loss: 1.4277\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 1.3716\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 1.3248\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 1.2865\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 1.2503\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 1.2152\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 1.1801\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 15s 84ms/step - loss: 1.1427\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 1.1068\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 14s 83ms/step - loss: 1.0675\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 1.0270\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.9865\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.9454\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.9039\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.8624\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 15s 84ms/step - loss: 0.8241\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.7875\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.7517\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 15s 84ms/step - loss: 0.7182\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 15s 84ms/step - loss: 0.6915\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 14s 83ms/step - loss: 0.6651\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.6401\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.6169\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.5957\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.5783\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.5629\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.5470\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 14s 83ms/step - loss: 0.5345\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.5219\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.5108\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 14s 83ms/step - loss: 0.5022\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.4925\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 15s 84ms/step - loss: 0.4846\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.4769\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 14s 83ms/step - loss: 0.4707\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.4648\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 15s 84ms/step - loss: 0.4595\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.4550\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 15s 86ms/step - loss: 0.4499\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.4482\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.4409\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.4388\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.4332\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.4325\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 14s 84ms/step - loss: 0.4311\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 15s 85ms/step - loss: 0.4266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhZOEz3IkawM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCrUUB-KmtuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEdKg68-my2h",
        "colab_type": "text"
      },
      "source": [
        "## Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYx0E-MgmuOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "1f50b911-d4ec-455d-d5b8-4fded7c5bda2"
      },
      "source": [
        "drama = generate_text(model, \"Sky\")\n",
        "\n",
        "print(drama)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sky are thus.\n",
            "\n",
            "MARIANA:\n",
            "I call'd thee gazer on between my soul--\n",
            "Soldove\n",
            "That I deliver to her by my sharous to the wron, I like it not.\n",
            "Set up the meat cortaining and your husband.\n",
            "\n",
            "Lord:\n",
            "Thou art a foot, a queen, or I'll speak with her, and welcome he:\n",
            "He loves me to behimpt and fetch shole,\n",
            "And his embassador. Let me have ways,\n",
            "The next receven unexpress'd it\n",
            "Rou any princely passes,\n",
            "But o'er his worth and honour no more.\n",
            "\n",
            "LUCIO:\n",
            "Well, first, you are stay\n",
            "Let him be heard; and with thy trumpers' headstroy yourself and to speak\n",
            "Against my fouler to the sun his streams,\n",
            "And then expent me, let him all; then from my other father, sir:\n",
            "But 'twas thy heavy lack of a cewn,\n",
            "To bous what lies excuse that thou hast marr'd;\n",
            "Thing here we loved him in a heart do tell it out,\n",
            "and then have found itsel\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}